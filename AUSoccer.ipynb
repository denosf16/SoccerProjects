{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fetching Team Names ===\n",
      "Team Names saved to: C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\\team_names.csv\n",
      "\n",
      "=== Fetching Player Names ===\n",
      "Player Names saved to: C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\\player_names.csv\n",
      "\n",
      "=== Fetching Match Data ===\n",
      "Enriched Match Information saved to: C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\\enriched_match_information.csv\n",
      "\n",
      "=== Fetching Player Salaries ===\n",
      "Enriched Player Information saved to: C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\\enriched_player_information.csv\n",
      "\n",
      "=== Fetching Team Salaries ===\n",
      "Team Information saved to: C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\\team_information.csv\n",
      "\n",
      "=== Fetching Player Statistics ===\n",
      "Player Statistics saved to: C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\\player_statistics.csv\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "from itscalledsoccer.client import AmericanSoccerAnalysis\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Initialize ASA Client\n",
    "asa_client = AmericanSoccerAnalysis()\n",
    "\n",
    "# Set Export Directory\n",
    "export_dir = r\"C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\"\n",
    "os.makedirs(export_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "# === Fetch Team Names ===\n",
    "try:\n",
    "    print(\"\\n=== Fetching Team Names ===\")\n",
    "    teams = asa_client.get_teams(leagues=\"mls\")\n",
    "    team_names_df = pd.DataFrame(teams)\n",
    "    team_names_csv_path = os.path.join(export_dir, \"team_names.csv\")\n",
    "    team_names_df.to_csv(team_names_csv_path, index=False)\n",
    "    print(f\"Team Names saved to: {team_names_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(\"Error fetching team names:\", e)\n",
    "\n",
    "# === Fetch Player Names ===\n",
    "try:\n",
    "    print(\"\\n=== Fetching Player Names ===\")\n",
    "    players = asa_client.get_players(leagues=\"mls\")\n",
    "    player_names_df = pd.DataFrame(players)\n",
    "    player_names_csv_path = os.path.join(export_dir, \"player_names.csv\")\n",
    "    player_names_df.to_csv(player_names_csv_path, index=False)\n",
    "    print(f\"Player Names saved to: {player_names_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(\"Error fetching player names:\", e)\n",
    "\n",
    "# === Match Information Dataset ===\n",
    "try:\n",
    "    print(\"\\n=== Fetching Match Data ===\")\n",
    "    matches = asa_client.get_games()\n",
    "    match_df = pd.DataFrame(matches)\n",
    "    match_columns = ['game_id', 'date_time_utc', 'home_score', 'away_score', \n",
    "                     'home_team_id', 'away_team_id', 'season_name', 'attendance']\n",
    "    match_df = match_df[[col for col in match_columns if col in match_df.columns]]\n",
    "\n",
    "    # Enrich Match Data with Team Names\n",
    "    if 'team_id' in team_names_df.columns:\n",
    "        match_df = match_df.merge(team_names_df[['team_id', 'team_name']], \n",
    "                                  left_on='home_team_id', right_on='team_id', how='left').rename(columns={'team_name': 'home_team_name'})\n",
    "        match_df = match_df.merge(team_names_df[['team_id', 'team_name']], \n",
    "                                  left_on='away_team_id', right_on='team_id', how='left').rename(columns={'team_name': 'away_team_name'})\n",
    "\n",
    "    match_csv_path = os.path.join(export_dir, \"enriched_match_information.csv\")\n",
    "    match_df.to_csv(match_csv_path, index=False)\n",
    "    print(f\"Enriched Match Information saved to: {match_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(\"Error fetching match data:\", e)\n",
    "\n",
    "# === Player Information Dataset (with Salaries) ===\n",
    "try:\n",
    "    print(\"\\n=== Fetching Player Salaries ===\")\n",
    "    player_salaries = asa_client.get_player_salaries(leagues=\"mls\")\n",
    "    player_columns = ['player_id', 'team_id', 'season_name', 'position', \n",
    "                      'base_salary', 'guaranteed_compensation', 'mlspa_release']\n",
    "    player_info_df = player_salaries[[col for col in player_columns if col in player_salaries.columns]]\n",
    "\n",
    "    # Enrich Player Information with Player Names\n",
    "    if 'player_id' in player_info_df.columns and 'player_id' in player_names_df.columns:\n",
    "        player_info_df = player_info_df.merge(player_names_df[['player_id', 'player_name']], on='player_id', how='left')\n",
    "\n",
    "    player_info_csv_path = os.path.join(export_dir, \"enriched_player_information.csv\")\n",
    "    player_info_df.to_csv(player_info_csv_path, index=False)\n",
    "    print(f\"Enriched Player Information saved to: {player_info_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(\"Error fetching player salary data:\", e)\n",
    "\n",
    "# === Team Information Dataset (with Salaries) ===\n",
    "try:\n",
    "    print(\"\\n=== Fetching Team Salaries ===\")\n",
    "    team_salaries = asa_client.get_team_salaries(leagues=\"mls\", split_by_seasons=True)\n",
    "    team_info_csv_path = os.path.join(export_dir, \"team_information.csv\")\n",
    "    team_salaries.to_csv(team_info_csv_path, index=False)\n",
    "    print(f\"Team Information saved to: {team_info_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(\"Error fetching team salary data:\", e)\n",
    "\n",
    "# === Player Statistics Dataset ===\n",
    "try:\n",
    "    print(\"\\n=== Fetching Player Statistics ===\")\n",
    "    player_stats = asa_client.get_player_xgoals(leagues=\"mls\", split_by_seasons=True)\n",
    "    player_stats_df = pd.DataFrame(player_stats)\n",
    "    player_stats_csv_path = os.path.join(export_dir, \"player_statistics.csv\")\n",
    "    player_stats_df.to_csv(player_stats_csv_path, index=False)\n",
    "    print(f\"Player Statistics saved to: {player_stats_csv_path}\")\n",
    "except Exception as e:\n",
    "    print(\"Error fetching player statistics:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from datetime import datetime\n",
    "\n",
    "# Set Export Directory\n",
    "export_dir = r\"C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# Define a dictionary to log changes for each dataset\n",
    "change_logs = {}\n",
    "\n",
    "# Function to log changes\n",
    "def log_change(dataset_name, description):\n",
    "    if dataset_name not in change_logs:\n",
    "        change_logs[dataset_name] = []\n",
    "    change_logs[dataset_name].append(f\"{datetime.now()} - {description}\")\n",
    "\n",
    "# Function to handle duplicate rows\n",
    "def handle_duplicate_rows(df, dataset_name):\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    log_change(dataset_name, f\"Detected {duplicate_count} duplicate rows.\")\n",
    "    df = df.drop_duplicates()\n",
    "    log_change(dataset_name, \"Removed duplicate rows.\")\n",
    "    return df\n",
    "\n",
    "# Function to handle duplicate columns\n",
    "def handle_duplicate_columns(df, dataset_name):\n",
    "    duplicate_columns = []\n",
    "    for i, col1 in enumerate(df.columns):\n",
    "        for j, col2 in enumerate(df.columns):\n",
    "            if i < j and df[col1].equals(df[col2]):\n",
    "                duplicate_columns.append((col1, col2))\n",
    "    log_change(dataset_name, f\"Detected {len(duplicate_columns)} duplicate columns: {duplicate_columns}\")\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    log_change(dataset_name, \"Removed duplicate columns.\")\n",
    "    return df\n",
    "\n",
    "# Function to handle outliers\n",
    "def handle_outliers(df, dataset_name, numeric_cols, z_threshold=3):\n",
    "    z_scores = df[numeric_cols].apply(zscore)\n",
    "    for col in numeric_cols:\n",
    "        outlier_col = f\"{col}_outlier\"\n",
    "        df[outlier_col] = (z_scores[col].abs() > z_threshold)\n",
    "        log_change(dataset_name, f\"Detected outliers in column '{col}': {df[outlier_col].sum()} flagged as outliers.\")\n",
    "    return df\n",
    "\n",
    "# Function to calculate player ages\n",
    "def calculate_player_ages(player_names_df):\n",
    "    player_names_df['birth_date'] = pd.to_datetime(player_names_df['birth_date'])\n",
    "    player_names_df['age'] = (datetime.now() - player_names_df['birth_date']).dt.days // 365\n",
    "    return player_names_df\n",
    "\n",
    "# Dataset paths\n",
    "datasets = {\n",
    "    \"player_statistics\": os.path.join(export_dir, \"player_statistics.csv\"),\n",
    "    \"enriched_player_information\": os.path.join(export_dir, \"enriched_player_information.csv\"),\n",
    "    \"enriched_match_information\": os.path.join(export_dir, \"enriched_match_information.csv\"),\n",
    "    \"player_names\": os.path.join(export_dir, \"player_names.csv\"),\n",
    "}\n",
    "\n",
    "processed_datasets = {}\n",
    "for dataset_name, file_path in datasets.items():\n",
    "    try:\n",
    "        # Load dataset\n",
    "        print(f\"Processing {dataset_name}...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Remove duplicate rows and columns\n",
    "        df = handle_duplicate_rows(df, dataset_name)\n",
    "        df = handle_duplicate_columns(df, dataset_name)\n",
    "\n",
    "        # Apply outlier handling where specified\n",
    "        if dataset_name == \"player_statistics\":\n",
    "            numeric_cols = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "            df = handle_outliers(df, dataset_name, numeric_cols)\n",
    "        elif dataset_name == \"enriched_player_information\":\n",
    "            numeric_cols = [\"base_salary\", \"guaranteed_compensation\"]\n",
    "            df = handle_outliers(df, dataset_name, numeric_cols)\n",
    "        elif dataset_name == \"enriched_match_information\":\n",
    "            numeric_cols = [\"home_score\", \"away_score\"]\n",
    "            df = handle_outliers(df, dataset_name, numeric_cols)\n",
    "\n",
    "        # Calculate player ages for player_names dataset\n",
    "        if dataset_name == \"player_names\":\n",
    "            df = calculate_player_ages(df)\n",
    "\n",
    "        # Save the processed dataset\n",
    "        output_path = os.path.join(export_dir, f\"processed_{dataset_name}.csv\")\n",
    "        df.to_csv(output_path, index=False)\n",
    "        processed_datasets[dataset_name] = df\n",
    "        log_change(dataset_name, f\"Processed dataset saved to {output_path}.\")\n",
    "    except Exception as e:\n",
    "        log_change(dataset_name, f\"Error processing dataset: {e}\")\n",
    "        print(f\"Error processing {dataset_name}: {e}\")\n",
    "\n",
    "# Save change logs\n",
    "log_path = os.path.join(export_dir, \"change_logs.txt\")\n",
    "with open(log_path, \"w\") as f:\n",
    "    for dataset_name, logs in change_logs.items():\n",
    "        f.write(f\"=== Changes for {dataset_name} ===\\n\")\n",
    "        f.write(\"\\n\".join(logs))\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "print(\"\\nProcessing completed. Logs saved to:\", log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing processed_enriched_match_information for 2024 data...\n",
      "Filtered 2024 data saved to: C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\\processed_enriched_match_information_2024.csv\n",
      "Processing processed_enriched_player_information for 2024 data...\n",
      "Filtered 2024 data saved to: C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\\processed_enriched_player_information_2024.csv\n",
      "Processing processed_player_statistics for 2024 data...\n",
      "Filtered 2024 data saved to: C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\\processed_player_statistics_2024.csv\n",
      "\n",
      "Filtered datasets for 2024:\n",
      "  - processed_enriched_match_information: 190 rows\n",
      "  - processed_enriched_player_information: 1786 rows\n",
      "  - processed_player_statistics: 801 rows\n"
     ]
    }
   ],
   "source": [
    "# Set Export Directory\n",
    "export_dir = r\"C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# Filenames for processed datasets\n",
    "datasets = {\n",
    "    \"processed_enriched_match_information\": os.path.join(export_dir, \"processed_enriched_match_information.csv\"),\n",
    "    \"processed_enriched_player_information\": os.path.join(export_dir, \"processed_enriched_player_information.csv\"),\n",
    "    \"processed_player_statistics\": os.path.join(export_dir, \"processed_player_statistics.csv\"),\n",
    "}\n",
    "\n",
    "# Create filtered datasets for 2024\n",
    "filtered_datasets = {}\n",
    "for dataset_name, file_path in datasets.items():\n",
    "    try:\n",
    "        print(f\"Processing {dataset_name} for 2024 data...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Ensure season_name is treated as numeric and filter for 2024\n",
    "        if \"season_name\" in df.columns:\n",
    "            df[\"season_name\"] = pd.to_numeric(df[\"season_name\"], errors=\"coerce\")\n",
    "            filtered_df = df[df[\"season_name\"] == 2024]\n",
    "            filtered_datasets[dataset_name] = filtered_df\n",
    "            \n",
    "            # Save the filtered dataset\n",
    "            output_path = os.path.join(export_dir, f\"{dataset_name}_2024.csv\")\n",
    "            filtered_df.to_csv(output_path, index=False)\n",
    "            print(f\"Filtered 2024 data saved to: {output_path}\")\n",
    "        else:\n",
    "            print(f\"'season_name' column not found in {dataset_name}. Skipping...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {dataset_name}: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nFiltered datasets for 2024:\")\n",
    "for dataset_name, df in filtered_datasets.items():\n",
    "    print(f\"  - {dataset_name}: {len(df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No issues detected in non-numeric columns for MatchInfo\n",
      "Cleaned non-numeric dataset saved: C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\\processed_enriched_match_information_2024_cleaned_non_numeric.csv\n",
      "Issues detected and logged for PlayerSalary\n",
      "Cleaned non-numeric dataset saved: C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\\processed_enriched_player_information_2024_cleaned_non_numeric.csv\n",
      "Issues detected and logged for PlayerInfo\n",
      "Cleaned non-numeric dataset saved: C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\\processed_player_names_cleaned_non_numeric.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set file paths\n",
    "file_paths = {\n",
    "    \"processed_enriched_match_information_2024.csv\": \"MatchInfo\",\n",
    "    \"processed_enriched_player_information_2024.csv\": \"PlayerSalary\",\n",
    "    \"processed_player_names.csv\": \"PlayerInfo\"\n",
    "}\n",
    "\n",
    "# Set log directory\n",
    "log_dir = r\"C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\\logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Function to validate and clean non-numeric columns\n",
    "def validate_and_clean_non_numeric_columns(df, dataset_name):\n",
    "    issue_log = []\n",
    "\n",
    "    # Iterate over non-numeric columns\n",
    "    for col in df.select_dtypes(include=[object]).columns:\n",
    "        original_count = len(df)\n",
    "\n",
    "        # Handle empty strings and NaN\n",
    "        empty_strings = df[col] == \"\"\n",
    "        if empty_strings.sum() > 0:\n",
    "            issue_log.append(f\"{col}: {empty_strings.sum()} empty string values found\")\n",
    "            df[col] = df[col].replace(\"\", pd.NA)  # Replace empty strings with NaN\n",
    "\n",
    "        # Handle NaN values\n",
    "        nan_values = df[col].isna().sum()\n",
    "        if nan_values > 0:\n",
    "            issue_log.append(f\"{col}: {nan_values} NaN values found\")\n",
    "\n",
    "        # Normalize specific columns (e.g., player names)\n",
    "        if col == \"player_name\":  # Example: Normalize player names with special characters\n",
    "            df[col] = df[col].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "        # Check for unusual formats in season_name (e.g., empty sets or lists)\n",
    "        if col == \"season_name\":\n",
    "            df[col] = df[col].apply(lambda x: str(x) if isinstance(x, str) else \"Invalid\")\n",
    "\n",
    "    # Log detected issues\n",
    "    if issue_log:\n",
    "        with open(os.path.join(log_dir, f\"{dataset_name}_non_numeric_issues.log\"), \"w\") as log_file:\n",
    "            log_file.write(\"\\n\".join(issue_log))\n",
    "        print(f\"Issues detected and logged for {dataset_name}\")\n",
    "    else:\n",
    "        print(f\"No issues detected in non-numeric columns for {dataset_name}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to clean specific datasets\n",
    "def clean_and_save_non_numeric_dataset(file_path, dataset_name):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Clean non-numeric columns and log issues\n",
    "    df = validate_and_clean_non_numeric_columns(df, dataset_name)\n",
    "\n",
    "    # Save the cleaned dataset\n",
    "    cleaned_file_path = file_path.replace(\".csv\", \"_cleaned_non_numeric.csv\")\n",
    "    df.to_csv(cleaned_file_path, index=False)\n",
    "    print(f\"Cleaned non-numeric dataset saved: {cleaned_file_path}\")\n",
    "\n",
    "# Loop over datasets and process them\n",
    "for file_name, dataset_name in file_paths.items():\n",
    "    file_path = os.path.join(r\"C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\", file_name)\n",
    "    try:\n",
    "        clean_and_save_non_numeric_dataset(file_path, dataset_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {dataset_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issues detected and logged for MatchInfo\n",
      "Dropped existing table: MatchInfo\n",
      "Data uploaded to MatchInfo in SQL Server.\n",
      "Issues detected and logged for PlayerSalary\n",
      "Dropped existing table: PlayerSalary\n",
      "Error processing PlayerSalary: ('42000', '[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]The incoming tabular data stream (TDS) remote procedure call (RPC) protocol stream is incorrect. Parameter 7 (\"\"): The supplied value is not a valid instance of data type float. Check the source data for invalid values. An example of an invalid value is data of numeric type with scale greater than precision. (8023) (SQLExecDirectW)')\n",
      "Issues detected and logged for PlayerInfo\n",
      "Dropped existing table: PlayerInfo\n",
      "Error processing PlayerInfo: ('42000', '[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]The incoming tabular data stream (TDS) remote procedure call (RPC) protocol stream is incorrect. Parameter 8 (\"\"): The supplied value is not a valid instance of data type float. Check the source data for invalid values. An example of an invalid value is data of numeric type with scale greater than precision. (8023) (SQLExecDirectW)')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import os\n",
    "\n",
    "# Set file paths\n",
    "file_paths = {\n",
    "    \"processed_enriched_match_information_2024.csv\": \"MatchInfo\",\n",
    "    \"processed_enriched_player_information_2024.csv\": \"PlayerSalary\",\n",
    "    \"processed_player_names.csv\": \"PlayerInfo\"\n",
    "}\n",
    "\n",
    "# Set log directory\n",
    "log_dir = r\"C:\\Users\\denos\\OneDrive\\Projects\\AU_Soccer\\logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Function to clean and validate non-numeric columns\n",
    "def clean_and_validate_non_numeric_columns(df, dataset_name):\n",
    "    issue_log = []\n",
    "    # Remove empty columns\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    issue_log.append(f\"Removed empty columns from {dataset_name}\")\n",
    "\n",
    "    # Handle empty strings and NaNs in non-numeric columns\n",
    "    for col in df.select_dtypes(include=[object]).columns:\n",
    "        empty_strings = df[col] == \"\"\n",
    "        if empty_strings.sum() > 0:\n",
    "            issue_log.append(f\"{col}: {empty_strings.sum()} empty string values replaced.\")\n",
    "            df[col] = df[col].replace(\"\", pd.NA)\n",
    "\n",
    "        # Log NaNs in columns\n",
    "        nan_values = df[col].isna().sum()\n",
    "        if nan_values > 0:\n",
    "            issue_log.append(f\"{col}: {nan_values} NaN values found\")\n",
    "\n",
    "    # Log issues to a file\n",
    "    if issue_log:\n",
    "        with open(os.path.join(log_dir, f\"{dataset_name}_non_numeric_issues.log\"), \"w\") as log_file:\n",
    "            log_file.write(\"\\n\".join(issue_log))\n",
    "        print(f\"Issues detected and logged for {dataset_name}\")\n",
    "    return df\n",
    "\n",
    "# Function to upload the cleaned data to SQL Server\n",
    "def upload_to_sql(df, table_name, server, database):\n",
    "    # Create SQL connection string\n",
    "    connection_string = f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;\"\n",
    "    conn = pyodbc.connect(connection_string)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Drop existing table if it exists\n",
    "    cursor.execute(f\"IF OBJECT_ID('{table_name}', 'U') IS NOT NULL DROP TABLE {table_name}\")\n",
    "    conn.commit()\n",
    "    print(f\"Dropped existing table: {table_name}\")\n",
    "\n",
    "    # Create table dynamically\n",
    "    create_table_query = f\"CREATE TABLE {table_name} ({', '.join([f'[{col}] NVARCHAR(MAX)' for col in df.columns])});\"\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "\n",
    "    # Insert data\n",
    "    for _, row in df.iterrows():\n",
    "        placeholders = \", \".join([\"?\" for _ in row])\n",
    "        insert_query = f\"INSERT INTO {table_name} VALUES ({placeholders})\"\n",
    "        cursor.execute(insert_query, *row)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"Data uploaded to {table_name} in SQL Server.\")\n",
    "\n",
    "# Loop through each dataset and clean/upload\n",
    "for file_name, dataset_name in file_paths.items():\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        file_path = f\"C:\\\\Users\\\\denos\\\\OneDrive\\\\Projects\\\\AU_Soccer\\\\{file_name}\"\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Clean non-numeric columns\n",
    "        df = clean_and_validate_non_numeric_columns(df, dataset_name)\n",
    "\n",
    "        # Upload cleaned data to SQL Server\n",
    "        upload_to_sql(df, dataset_name, \"RAMSEY_BOLTON\\\\SQLEXPRESS\", \"SoccerProjects\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {dataset_name}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
