{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing team_names...\n",
      "\n",
      "Processing player_names...\n",
      "\n",
      "Processing match_data...\n",
      "\n",
      "Processing player_stats...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denos\\AppData\\Local\\Temp\\ipykernel_24832\\2766459382.py:22: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  problematic_rows = df.applymap(lambda x: isinstance(x, (dict, list))).any(axis=1)\n",
      "C:\\Users\\denos\\AppData\\Local\\Temp\\ipykernel_24832\\2766459382.py:55: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[numeric_cols] = df[numeric_cols].applymap(lambda x: None if pd.isna(x) else x)\n",
      "C:\\Users\\denos\\AppData\\Local\\Temp\\ipykernel_24832\\2766459382.py:22: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  problematic_rows = df.applymap(lambda x: isinstance(x, (dict, list))).any(axis=1)\n",
      "C:\\Users\\denos\\AppData\\Local\\Temp\\ipykernel_24832\\2766459382.py:55: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[numeric_cols] = df[numeric_cols].applymap(lambda x: None if pd.isna(x) else x)\n",
      "C:\\Users\\denos\\AppData\\Local\\Temp\\ipykernel_24832\\2766459382.py:22: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  problematic_rows = df.applymap(lambda x: isinstance(x, (dict, list))).any(axis=1)\n",
      "C:\\Users\\denos\\AppData\\Local\\Temp\\ipykernel_24832\\2766459382.py:55: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[numeric_cols] = df[numeric_cols].applymap(lambda x: None if pd.isna(x) else x)\n",
      "C:\\Users\\denos\\AppData\\Local\\Temp\\ipykernel_24832\\2766459382.py:22: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  problematic_rows = df.applymap(lambda x: isinstance(x, (dict, list))).any(axis=1)\n",
      "C:\\Users\\denos\\AppData\\Local\\Temp\\ipykernel_24832\\2766459382.py:55: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[numeric_cols] = df[numeric_cols].applymap(lambda x: None if pd.isna(x) else x)\n",
      "C:\\Users\\denos\\AppData\\Local\\Temp\\ipykernel_24832\\2766459382.py:22: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  problematic_rows = df.applymap(lambda x: isinstance(x, (dict, list))).any(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing player_salaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denos\\AppData\\Local\\Temp\\ipykernel_24832\\2766459382.py:55: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[numeric_cols] = df[numeric_cols].applymap(lambda x: None if pd.isna(x) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to SQL Server table: AU_team_names\n",
      "Data uploaded to SQL Server table: AU_player_names\n",
      "Data uploaded to SQL Server table: AU_match_data\n",
      "Data uploaded to SQL Server table: AU_player_stats\n",
      "Data uploaded to SQL Server table: AU_player_salaries\n",
      "\n",
      "Processing and Uploading Change Logs...\n",
      "Change logs saved to C:\\\\Users\\\\denos\\\\OneDrive\\\\Projects\\\\AU_Soccer\\AU_change_logs.csv\n",
      "Data uploaded to SQL Server table: AU_ChangeLogs\n",
      "Change logs uploaded to SQL Server.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pyodbc\n",
    "\n",
    "# Set Export Directory\n",
    "export_dir = r\"C:\\\\Users\\\\denos\\\\OneDrive\\\\Projects\\\\AU_Soccer\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# Define a dictionary to log changes\n",
    "change_logs = {}\n",
    "\n",
    "# Function to log changes\n",
    "def log_change(dataset_name, description):\n",
    "    if dataset_name not in change_logs:\n",
    "        change_logs[dataset_name] = []\n",
    "    change_logs[dataset_name].append(f\"{datetime.now()} - {description}\")\n",
    "\n",
    "# Function to handle duplicate rows\n",
    "def handle_duplicate_rows(df, dataset_name):\n",
    "    problematic_rows = df.applymap(lambda x: isinstance(x, (dict, list))).any(axis=1)\n",
    "    if problematic_rows.any():\n",
    "        count = problematic_rows.sum()\n",
    "        log_change(dataset_name, f\"Detected {count} rows with unhashable types. Dropping these rows.\")\n",
    "        df = df[~problematic_rows]\n",
    "\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    log_change(dataset_name, f\"Detected {duplicate_count} duplicate rows.\")\n",
    "    df = df.drop_duplicates()\n",
    "    log_change(dataset_name, \"Removed duplicate rows.\")\n",
    "    return df\n",
    "\n",
    "# Function to handle duplicate columns\n",
    "def handle_duplicate_columns(df, dataset_name):\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    log_change(dataset_name, \"Removed duplicate columns.\")\n",
    "    return df\n",
    "\n",
    "# Function to drop unwanted columns\n",
    "def drop_unwanted_columns(df, unwanted_columns, dataset_name):\n",
    "    df = df.drop(columns=[col for col in unwanted_columns if col in df.columns], errors=\"ignore\")\n",
    "    log_change(dataset_name, f\"Dropped unwanted columns: {unwanted_columns}\")\n",
    "    return df\n",
    "\n",
    "# Clean dataset function\n",
    "def clean_dataset(df, dataset_name):\n",
    "    # Identify and drop unwanted columns\n",
    "    unwanted_columns = [\"extra_time\", \"penalties\", \"home_penalties\", \"away_penalties\", \"height_ft\", \"height_in\", \"weight_lb\"]\n",
    "    unwanted_columns += [col for col in df.columns if \"outlier\" in col]\n",
    "    df = drop_unwanted_columns(df, unwanted_columns, dataset_name)\n",
    "\n",
    "    # Replace NaN in numeric columns with None (SQL NULL)\n",
    "    numeric_cols = df.select_dtypes(include=[\"float64\"]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].applymap(lambda x: None if pd.isna(x) else x)\n",
    "\n",
    "    # Ensure all remaining columns are valid\n",
    "    df = validate_and_cast_data(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to validate and cast data for SQL upload\n",
    "def validate_and_cast_data(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"float64\":\n",
    "            df[col] = df[col].apply(lambda x: round(x, 6) if pd.notnull(x) and np.isfinite(x) else None)\n",
    "        elif df[col].dtype == \"object\":\n",
    "            df[col] = df[col].apply(lambda x: str(x) if isinstance(x, (str, int, float)) else None)\n",
    "    return df\n",
    "\n",
    "# Function to upload datasets to SQL Server\n",
    "def upload_to_sql(df, table_name, server, database):\n",
    "    connection_string = f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;\"\n",
    "    conn = pyodbc.connect(connection_string)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(f\"IF OBJECT_ID('{table_name}', 'U') IS NOT NULL DROP TABLE {table_name}\")\n",
    "    conn.commit()\n",
    "    create_query = f\"CREATE TABLE {table_name} ({', '.join([f'[{col}] NVARCHAR(MAX)' for col in df.columns])})\"\n",
    "    cursor.execute(create_query)\n",
    "    conn.commit()\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        placeholders = \", \".join([\"?\" for _ in row])\n",
    "        insert_query = f\"INSERT INTO {table_name} VALUES ({placeholders})\"\n",
    "        try:\n",
    "            cursor.execute(insert_query, *row)\n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting row {idx}: {row.to_dict()}\")\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"Data uploaded to SQL Server table: {table_name}\")\n",
    "\n",
    "# === Fetch All Datasets ===\n",
    "dataset_paths = {\n",
    "    \"team_names\": r\"C:\\\\Users\\\\denos\\\\OneDrive\\\\Projects\\\\AU_Soccer\\\\AU_processed_team_names.csv\",\n",
    "    \"player_names\": r\"C:\\\\Users\\\\denos\\\\OneDrive\\\\Projects\\\\AU_Soccer\\\\AU_processed_player_names.csv\",\n",
    "    \"match_data\": r\"C:\\\\Users\\\\denos\\\\OneDrive\\\\Projects\\\\AU_Soccer\\\\AU_processed_match_data.csv\",\n",
    "    \"player_stats\": r\"C:\\\\Users\\\\denos\\\\OneDrive\\\\Projects\\\\AU_Soccer\\\\AU_processed_player_stats.csv\",\n",
    "    \"player_salaries\": r\"C:\\\\Users\\\\denos\\\\OneDrive\\\\Projects\\\\AU_Soccer\\\\AU_processed_player_salaries.csv\",\n",
    "}\n",
    "\n",
    "processed_datasets = {}\n",
    "for dataset_name, path in dataset_paths.items():\n",
    "    print(f\"\\nProcessing {dataset_name}...\")\n",
    "    df = pd.read_csv(path)\n",
    "    df = handle_duplicate_rows(df, dataset_name)\n",
    "    df = handle_duplicate_columns(df, dataset_name)\n",
    "    df = clean_dataset(df, dataset_name)\n",
    "    processed_datasets[dataset_name] = df\n",
    "\n",
    "# === Save and Upload Datasets ===\n",
    "for dataset_name, df in processed_datasets.items():\n",
    "    table_name = f\"AU_{dataset_name}\"\n",
    "    upload_to_sql(df, table_name, \"RAMSEY_BOLTON\\\\SQLEXPRESS\", \"SoccerProjects\")\n",
    "\n",
    "# === Process and Upload Change Logs ===\n",
    "print(\"\\nProcessing and Uploading Change Logs...\")\n",
    "\n",
    "change_logs_list = []\n",
    "for dataset_name, logs in change_logs.items():\n",
    "    for log in logs:\n",
    "        truncated_log = log[:4000] if len(log) > 4000 else log\n",
    "        change_logs_list.append({\"Dataset\": dataset_name, \"LogEntry\": truncated_log})\n",
    "\n",
    "change_logs_df = pd.DataFrame(change_logs_list)\n",
    "\n",
    "if not change_logs_df.empty:\n",
    "    change_logs_df = validate_and_cast_data(change_logs_df)\n",
    "    change_logs_path = os.path.join(export_dir, \"AU_change_logs.csv\")\n",
    "    change_logs_df.to_csv(change_logs_path, index=False)\n",
    "    print(f\"Change logs saved to {change_logs_path}\")\n",
    "\n",
    "    upload_to_sql(change_logs_df, \"AU_ChangeLogs\", \"RAMSEY_BOLTON\\\\SQLEXPRESS\", \"SoccerProjects\")\n",
    "    print(\"Change logs uploaded to SQL Server.\")\n",
    "else:\n",
    "    print(\"No changes were logged during this run.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
