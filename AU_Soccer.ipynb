{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChangeLogs table created or already exists.\n",
      "=== Endpoint: mls/players ===\n",
      "['player_id', 'player_name', 'birth_date', 'height_ft', 'height_in', 'weight_lb', 'nationality', 'primary_broad_position', 'primary_general_position', 'season_name', 'secondary_general_position', 'secondary_broad_position']\n",
      "Logged change for mls/players: Successfully fetched data from mls/players API endpoint.\n",
      "=== Endpoint: mls/players/xgoals ===\n",
      "['player_id', 'team_id', 'general_position', 'minutes_played', 'shots', 'shots_on_target', 'goals', 'xgoals', 'xplace', 'goals_minus_xgoals', 'key_passes', 'primary_assists', 'xassists', 'primary_assists_minus_xassists', 'xgoals_plus_xassists', 'points_added', 'xpoints_added']\n",
      "Logged change for mls/players/xgoals: Successfully fetched data from mls/players/xgoals API endpoint.\n",
      "=== Endpoint: mls/players/xpass ===\n",
      "['player_id', 'team_id', 'general_position', 'minutes_played', 'attempted_passes', 'pass_completion_percentage', 'xpass_completion_percentage', 'passes_completed_over_expected', 'passes_completed_over_expected_p100', 'avg_distance_yds', 'avg_vertical_distance_yds', 'share_team_touches', 'count_games']\n",
      "Logged change for mls/players/xpass: Successfully fetched data from mls/players/xpass API endpoint.\n",
      "=== Endpoint: mls/players/goals-added ===\n",
      "['player_id', 'team_id', 'general_position', 'minutes_played', 'data']\n",
      "Logged change for mls/players/goals-added: Successfully fetched data from mls/players/goals-added API endpoint.\n",
      "=== Endpoint: mls/players/salaries ===\n",
      "['player_id', 'team_id', 'season_name', 'position', 'base_salary', 'guaranteed_compensation', 'mlspa_release']\n",
      "Logged change for mls/players/salaries: Successfully fetched data from mls/players/salaries API endpoint.\n",
      "=== Endpoint: mls/goalkeepers/xgoals ===\n",
      "['player_id', 'team_id', 'minutes_played', 'shots_faced', 'goals_conceded', 'saves', 'share_headed_shots', 'xgoals_gk_faced', 'goals_minus_xgoals_gk', 'goals_divided_by_xgoals_gk']\n",
      "Logged change for mls/goalkeepers/xgoals: Successfully fetched data from mls/goalkeepers/xgoals API endpoint.\n",
      "=== Endpoint: mls/goalkeepers/goals-added ===\n",
      "['player_id', 'team_id', 'minutes_played', 'data']\n",
      "Logged change for mls/goalkeepers/goals-added: Successfully fetched data from mls/goalkeepers/goals-added API endpoint.\n",
      "=== Endpoint: mls/teams ===\n",
      "['team_id', 'team_name', 'team_short_name', 'team_abbreviation']\n",
      "Logged change for mls/teams: Successfully fetched data from mls/teams API endpoint.\n",
      "=== Endpoint: mls/teams/xgoals ===\n",
      "['team_id', 'count_games', 'shots_for', 'shots_against', 'goals_for', 'goals_against', 'goal_difference', 'xgoals_for', 'xgoals_against', 'xgoal_difference', 'goal_difference_minus_xgoal_difference', 'points', 'xpoints']\n",
      "Logged change for mls/teams/xgoals: Successfully fetched data from mls/teams/xgoals API endpoint.\n",
      "=== Endpoint: mls/teams/xpass ===\n",
      "['team_id', 'count_games', 'attempted_passes_for', 'pass_completion_percentage_for', 'xpass_completion_percentage_for', 'passes_completed_over_expected_for', 'passes_completed_over_expected_p100_for', 'avg_vertical_distance_for', 'attempted_passes_against', 'pass_completion_percentage_against', 'xpass_completion_percentage_against', 'passes_completed_over_expected_against', 'passes_completed_over_expected_p100_against', 'avg_vertical_distance_against', 'passes_completed_over_expected_difference', 'avg_vertical_distance_difference']\n",
      "Logged change for mls/teams/xpass: Successfully fetched data from mls/teams/xpass API endpoint.\n",
      "=== Endpoint: mls/teams/goals-added ===\n",
      "['team_id', 'minutes', 'data']\n",
      "Logged change for mls/teams/goals-added: Successfully fetched data from mls/teams/goals-added API endpoint.\n",
      "Error fetching mls/teams/salaries: 400 Client Error: Bad Request for url: https://app.americansocceranalysis.com/api/v1/mls/teams/salaries\n",
      "Logged change for mls/teams/salaries: Error fetching data from mls/teams/salaries: 400 Client Error: Bad Request for url: https://app.americansocceranalysis.com/api/v1/mls/teams/salaries\n",
      "=== Endpoint: mls/games ===\n",
      "['game_id', 'date_time_utc', 'home_score', 'away_score', 'home_team_id', 'away_team_id', 'referee_id', 'stadium_id', 'home_manager_id', 'away_manager_id', 'expanded_minutes', 'season_name', 'matchday', 'attendance', 'knockout_game', 'last_updated_utc', 'extra_time', 'penalties', 'home_penalties', 'away_penalties']\n",
      "Logged change for mls/games: Successfully fetched data from mls/games API endpoint.\n",
      "=== Endpoint: mls/games/xgoals ===\n",
      "['game_id', 'date_time_utc', 'home_team_id', 'home_goals', 'home_team_xgoals', 'home_player_xgoals', 'away_team_id', 'away_goals', 'away_team_xgoals', 'away_player_xgoals', 'goal_difference', 'team_xgoal_difference', 'player_xgoal_difference', 'final_score_difference', 'home_xpoints', 'away_xpoints']\n",
      "Logged change for mls/games/xgoals: Successfully fetched data from mls/games/xgoals API endpoint.\n",
      "=== Endpoint: mls/managers ===\n",
      "['manager_id', 'manager_name', 'nationality']\n",
      "Logged change for mls/managers: Successfully fetched data from mls/managers API endpoint.\n",
      "=== Endpoint: mls/referees ===\n",
      "['referee_id', 'referee_name', 'birth_date', 'nationality']\n",
      "Logged change for mls/referees: Successfully fetched data from mls/referees API endpoint.\n",
      "=== Endpoint: mls/stadia ===\n",
      "['stadium_id', 'stadium_name', 'capacity', 'year_built', 'roof', 'turf', 'street', 'city', 'province', 'country', 'postal_code', 'latitude', 'longitude', 'field_x', 'field_y']\n",
      "Logged change for mls/stadia: Successfully fetched data from mls/stadia API endpoint.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "# SQL Connection Setup\n",
    "conn = pyodbc.connect(f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server_name};DATABASE={database_name};Trusted_Connection=yes;\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Function to create ChangeLogs table if it doesn't exist\n",
    "def create_change_logs_table(conn, cursor):\n",
    "    cursor.execute(\"\"\"\n",
    "        IF OBJECT_ID('ChangeLogs', 'U') IS NULL\n",
    "        CREATE TABLE ChangeLogs (\n",
    "            id INT PRIMARY KEY IDENTITY(1,1),\n",
    "            dataset_name NVARCHAR(255),\n",
    "            description NVARCHAR(MAX),\n",
    "            timestamp DATETIME\n",
    "        );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    print(\"ChangeLogs table created or already exists.\")\n",
    "\n",
    "# Function to log changes\n",
    "def log_change(conn, cursor, dataset_name, description):\n",
    "    timestamp = pd.Timestamp.now()\n",
    "    try:\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO ChangeLogs (dataset_name, description, timestamp)\n",
    "            VALUES (?, ?, ?)\n",
    "        \"\"\", (dataset_name, description, timestamp))\n",
    "        conn.commit()\n",
    "        print(f\"Logged change for {dataset_name}: {description}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error logging change for {dataset_name}: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "# Step 1: Ensure ChangeLogs table exists\n",
    "create_change_logs_table(conn, cursor)\n",
    "\n",
    "# Base URL for ASA API\n",
    "BASE_URL = \"https://app.americansocceranalysis.com/api/v1\"\n",
    "\n",
    "# List of endpoints\n",
    "endpoints = [\n",
    "    \"mls/players\",\n",
    "    \"mls/players/xgoals\",\n",
    "    \"mls/players/xpass\",\n",
    "    \"mls/players/goals-added\",\n",
    "    \"mls/players/salaries\",\n",
    "    \"mls/goalkeepers/xgoals\",\n",
    "    \"mls/goalkeepers/goals-added\",\n",
    "    \"mls/teams\",\n",
    "    \"mls/teams/xgoals\",\n",
    "    \"mls/teams/xpass\",\n",
    "    \"mls/teams/goals-added\",\n",
    "    \"mls/teams/salaries\",\n",
    "    \"mls/games\",\n",
    "    \"mls/games/xgoals\",\n",
    "    \"mls/managers\",\n",
    "    \"mls/referees\",\n",
    "    \"mls/stadia\"\n",
    "]\n",
    "\n",
    "# Function to fetch and log data\n",
    "def fetch_key_fields(endpoint):\n",
    "    url = f\"{BASE_URL}/{endpoint}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses\n",
    "        data = response.json()\n",
    "        \n",
    "        if isinstance(data, list) and len(data) > 0:\n",
    "            key_fields = pd.json_normalize(data).columns.tolist()\n",
    "            print(f\"=== Endpoint: {endpoint} ===\")\n",
    "            print(key_fields)\n",
    "            \n",
    "            # Log the successful data fetch\n",
    "            log_change(conn, cursor, endpoint, f\"Successfully fetched data from {endpoint} API endpoint.\")\n",
    "            return key_fields\n",
    "        else:\n",
    "            print(f\"Endpoint {endpoint} returned unexpected data format.\")\n",
    "            log_change(conn, cursor, endpoint, f\"Error: Endpoint {endpoint} returned unexpected data format.\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {endpoint}: {e}\")\n",
    "        log_change(conn, cursor, endpoint, f\"Error fetching data from {endpoint}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Iterate over endpoints and fetch key fields\n",
    "endpoint_key_fields = {}\n",
    "for endpoint in endpoints:\n",
    "    key_fields = fetch_key_fields(endpoint)\n",
    "    endpoint_key_fields[endpoint] = key_fields\n",
    "\n",
    "# Display collected key fields for all endpoints\n",
    "endpoint_key_fields_df = pd.DataFrame.from_dict(endpoint_key_fields, orient=\"index\").T\n",
    "endpoint_key_fields_df.fillna(\"\", inplace=True)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from mls/players...\n",
      "Logged change for mls/players: Successfully fetched data from mls/players API endpoint.\n",
      "Fetching data from mls/players/xgoals...\n",
      "Logged change for mls/players/xgoals: Successfully fetched data from mls/players/xgoals API endpoint.\n",
      "Fetching data from mls/players/xpass...\n",
      "Logged change for mls/players/xpass: Successfully fetched data from mls/players/xpass API endpoint.\n",
      "Fetching data from mls/players/salaries...\n",
      "Logged change for mls/players/salaries: Successfully fetched data from mls/players/salaries API endpoint.\n",
      "Fetching data from mls/teams...\n",
      "Logged change for mls/teams: Successfully fetched data from mls/teams API endpoint.\n",
      "Fetching data from mls/games...\n",
      "Logged change for mls/games: Successfully fetched data from mls/games API endpoint.\n",
      "Players DataFrame preview:\n",
      "    player_id          player_name  birth_date primary_broad_position  \\\n",
      "0  0Oq6006M6D          Ugo Ihemelu  1983-04-03                    NaN   \n",
      "1  0Oq60APM6D      Jason Hernandez  1983-08-26                     DF   \n",
      "2  0Oq60VgM6D     Jeff Larentowicz  1983-08-05                     MF   \n",
      "3  0Oq624oPq6  Kalani Kossa-Rienzi  2002-06-27                     DF   \n",
      "4  0Oq62GYAq6        Oscar Herrera  2002-03-28                    NaN   \n",
      "\n",
      "  primary_general_position  \n",
      "0                      NaN  \n",
      "1                       CB  \n",
      "2                       DM  \n",
      "3                       FB  \n",
      "4                      NaN  \n",
      "\n",
      "XGoals DataFrame preview:\n",
      "    player_id                               team_id  minutes_played  shots  \\\n",
      "0  0Oq60APM6D  [kRQabn8MKZ, 0KPqjA456v, Vj58weDM8n]            9774      4   \n",
      "1  0Oq60VgM6D  [KAqBN0Vqbg, kaDQ0wRqEv, X0Oq66zq6D]           20303    196   \n",
      "2  0Oq624oPq6                            jYQJ19EqGR              19      0   \n",
      "3  0Oq630dXQ6              [kaDQ0wRqEv, pzeQZ6xQKw]            6970    135   \n",
      "4  0Oq632k7Q6              [pzeQZ6xQKw, jYQJ19EqGR]            3672     34   \n",
      "\n",
      "   shots_on_target  goals   xgoals  xplace  goals_minus_xgoals  key_passes  \\\n",
      "0                0      0   0.5459 -0.5459             -0.5459          11   \n",
      "1               57     19  22.4174 -3.6930             -3.4174          72   \n",
      "2                0      0   0.0000  0.0000              0.0000           0   \n",
      "3               62     13  22.8965 -1.7738             -9.8965          65   \n",
      "4                6      0   1.3820  0.0017             -1.3820          43   \n",
      "\n",
      "   primary_assists  xassists  primary_assists_minus_xassists  \\\n",
      "0                0    0.8302                         -0.8302   \n",
      "1                9    6.5441                          2.4559   \n",
      "2                0    0.0000                          0.0000   \n",
      "3                6    8.6230                         -2.6230   \n",
      "4                1    3.0055                         -2.0055   \n",
      "\n",
      "   xgoals_plus_xassists  points_added  xpoints_added  \n",
      "0                1.3761        0.0000         0.3505  \n",
      "1               28.9615        9.5505        14.3498  \n",
      "2                0.0000        0.0000         0.0000  \n",
      "3               31.5195        9.7030        14.9382  \n",
      "4                4.3875        0.0000         0.7881  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# Base URL for ASA API\n",
    "BASE_URL = \"https://app.americansocceranalysis.com/api/v1\"\n",
    "\n",
    "# Initialize connection and cursor before starting the data fetch and log\n",
    "conn = pyodbc.connect(f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server_name};DATABASE={database_name};Trusted_Connection=yes;\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Function to log changes to ChangeLogs table\n",
    "def log_change(conn, cursor, dataset_name, description):\n",
    "    timestamp = pd.Timestamp.now()\n",
    "    try:\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO ChangeLogs (dataset_name, description, timestamp)\n",
    "            VALUES (?, ?, ?)\n",
    "        \"\"\", (dataset_name, description, timestamp))\n",
    "        conn.commit()\n",
    "        print(f\"Logged change for {dataset_name}: {description}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error logging change for {dataset_name}: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "# Function to fetch data from the API and filter columns\n",
    "def fetch_data(endpoint, required_columns):\n",
    "    url = f\"{BASE_URL}/{endpoint}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Convert to DataFrame and filter for required columns\n",
    "        if isinstance(data, list) and len(data) > 0:\n",
    "            df = pd.json_normalize(data)\n",
    "            filtered_df = df[required_columns]\n",
    "            \n",
    "            # Log success\n",
    "            log_change(conn, cursor, endpoint, f\"Successfully fetched data from {endpoint} API endpoint.\")\n",
    "            return filtered_df\n",
    "        else:\n",
    "            print(f\"Endpoint {endpoint} returned unexpected data format.\")\n",
    "            \n",
    "            # Log failure for unexpected format\n",
    "            log_change(conn, cursor, endpoint, f\"Error: Endpoint {endpoint} returned unexpected data format.\")\n",
    "            return pd.DataFrame(columns=required_columns)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {endpoint}: {e}\")\n",
    "        \n",
    "        # Log failure for fetch error\n",
    "        log_change(conn, cursor, endpoint, f\"Error fetching data from {endpoint}: {e}\")\n",
    "        return pd.DataFrame(columns=required_columns)\n",
    "\n",
    "# Define endpoints and required columns\n",
    "endpoints_and_columns = {\n",
    "    \"mls/players\": [\n",
    "        \"player_id\", \"player_name\", \"birth_date\", \"primary_broad_position\", \"primary_general_position\"\n",
    "    ],\n",
    "    \"mls/players/xgoals\": [\n",
    "        \"player_id\", \"team_id\", \"minutes_played\", \"shots\", \"shots_on_target\",\n",
    "        \"goals\", \"xgoals\", \"xplace\", \"goals_minus_xgoals\", \"key_passes\",\n",
    "        \"primary_assists\", \"xassists\", \"primary_assists_minus_xassists\",\n",
    "        \"xgoals_plus_xassists\", \"points_added\", \"xpoints_added\"\n",
    "    ],\n",
    "    \"mls/players/xpass\": [\n",
    "        \"player_id\", \"team_id\", \"minutes_played\", \"attempted_passes\",\n",
    "        \"pass_completion_percentage\", \"xpass_completion_percentage\",\n",
    "        \"passes_completed_over_expected\", \"avg_distance_yds\"\n",
    "    ],\n",
    "    \"mls/players/salaries\": [\n",
    "        \"player_id\", \"team_id\", \"season_name\", \"position\",\n",
    "        \"base_salary\", \"guaranteed_compensation\"\n",
    "    ],\n",
    "    \"mls/teams\": [\n",
    "        \"team_id\", \"team_name\"\n",
    "    ],\n",
    "    \"mls/games\": [\n",
    "        \"game_id\", \"date_time_utc\", \"home_score\", \"away_score\",\n",
    "        \"home_team_id\", \"away_team_id\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Initialize dictionary to store DataFrames for each endpoint\n",
    "dataframes = {}\n",
    "\n",
    "# Fetch data for each endpoint and store in the dictionary\n",
    "for endpoint, columns in endpoints_and_columns.items():\n",
    "    print(f\"Fetching data from {endpoint}...\")\n",
    "    dataframes[endpoint] = fetch_data(endpoint, columns)\n",
    "\n",
    "# Example: Access specific DataFrame\n",
    "players_df = dataframes[\"mls/players\"]\n",
    "xgoals_df = dataframes[\"mls/players/xgoals\"]\n",
    "\n",
    "# Display or process data further\n",
    "print(\"Players DataFrame preview:\")\n",
    "print(players_df.head())\n",
    "print(\"\\nXGoals DataFrame preview:\")\n",
    "print(xgoals_df.head())\n",
    "\n",
    "# Close the connection after all data has been fetched and logged\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and validating mls/players...\n",
      "Cleaning and validating mls/players/xgoals...\n",
      "Cleaning and validating mls/players/xpass...\n",
      "Cleaning and validating mls/players/salaries...\n",
      "Cleaning and validating mls/teams...\n",
      "Cleaning and validating mls/games...\n",
      "Change Logs:\n",
      "Dataset: mls/players/xgoals\n",
      "  - 2024-12-05 16:16:58.161872 - Validated and kept only valid player_id entries.\n",
      "Dataset: mls/players/xpass\n",
      "  - 2024-12-05 16:16:58.178130 - Validated and kept only valid player_id entries.\n",
      "Dataset: mls/games\n",
      "  - 2024-12-05 16:16:58.188131 - Validated and kept only valid team_id entries.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a dictionary to log changes\n",
    "change_logs = {}\n",
    "\n",
    "# Function to log changes\n",
    "def log_change(dataset_name, description):\n",
    "    if dataset_name not in change_logs:\n",
    "        change_logs[dataset_name] = []\n",
    "    change_logs[dataset_name].append(f\"{pd.Timestamp.now()} - {description}\")\n",
    "\n",
    "# Function to clean and validate a dataset\n",
    "def clean_and_validate_dataset(df, dataset_name, dataframes):\n",
    "    if df is None:\n",
    "        return None  # Skip if DataFrame is missing\n",
    "\n",
    "    # 1. Handle Missing Values\n",
    "    if dataset_name == \"mls/players\":\n",
    "        if \"birth_date\" in df.columns:\n",
    "            missing_birthdate_count = df[\"birth_date\"].isnull().sum()\n",
    "            df.loc[:, \"birth_date\"] = df[\"birth_date\"].fillna(\"1900-01-01\")\n",
    "            df.loc[:, \"birth_date\"] = pd.to_datetime(df[\"birth_date\"], errors=\"coerce\")\n",
    "            if missing_birthdate_count > 0:\n",
    "                log_change(dataset_name, f\"Filled missing birth_date with '1900-01-01'. {missing_birthdate_count} rows updated.\")\n",
    "        for col in [\"primary_broad_position\", \"primary_general_position\"]:\n",
    "            if col in df.columns:\n",
    "                missing_position_count = df[col].isnull().sum()\n",
    "                df.loc[:, col] = df[col].fillna(\"UNKNOWN\")\n",
    "                if missing_position_count > 0:\n",
    "                    log_change(dataset_name, f\"Filled missing values in {col} with 'UNKNOWN'. {missing_position_count} rows updated.\")\n",
    "    if dataset_name == \"mls/players/salaries\":\n",
    "        if \"team_id\" in df.columns:\n",
    "            missing_team_id_count = df[\"team_id\"].isnull().sum()\n",
    "            df = df.dropna(subset=[\"team_id\"])\n",
    "            if missing_team_id_count > 0:\n",
    "                log_change(dataset_name, f\"Dropped {missing_team_id_count} rows with missing 'team_id'.\")\n",
    "\n",
    "    # 2. Handle Duplicates\n",
    "    problematic_rows = df.apply(lambda x: isinstance(x, (dict, list)), axis=1)\n",
    "    if problematic_rows.any():\n",
    "        count = problematic_rows.sum()\n",
    "        df = df[~problematic_rows]\n",
    "        log_change(dataset_name, f\"Dropped {count} rows with unhashable types.\")\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    if duplicate_count > 0:\n",
    "        df = df.drop_duplicates()\n",
    "        log_change(dataset_name, f\"Dropped {duplicate_count} duplicate rows.\")\n",
    "    duplicate_columns = df.columns[df.columns.duplicated()].tolist()\n",
    "    if duplicate_columns:\n",
    "        df = df.loc[:, ~df.columns.duplicated()]\n",
    "        log_change(dataset_name, f\"Dropped duplicate columns: {duplicate_columns}.\")\n",
    "\n",
    "    # 3. Validate Data Types and Cast for SQL Compatibility\n",
    "    for col in df.columns:\n",
    "        if col in [\"birth_date\", \"date_time_utc\"]:\n",
    "            df.loc[:, col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        elif pd.api.types.is_float_dtype(df[col]):\n",
    "            df.loc[:, col] = df[col].apply(lambda x: round(x, 6) if pd.notnull(x) and pd.api.types.is_number(x) else None)\n",
    "        elif pd.api.types.is_object_dtype(df[col]):\n",
    "            df.loc[:, col] = df[col].apply(lambda x: str(x) if isinstance(x, (str, int, float)) else None)\n",
    "        elif pd.api.types.is_integer_dtype(df[col]):\n",
    "            df.loc[:, col] = pd.to_numeric(df[col], downcast=\"integer\", errors=\"coerce\")\n",
    "\n",
    "    # 4. Drop Unwanted Columns\n",
    "    unwanted_columns = [\"extra_time\", \"penalties\", \"home_penalties\", \"away_penalties\"]\n",
    "    unwanted_columns += [col for col in df.columns if \"outlier\" in col]\n",
    "    dropped_columns = [col for col in unwanted_columns if col in df.columns]\n",
    "    if dropped_columns:\n",
    "        df = df.drop(columns=dropped_columns)\n",
    "        log_change(dataset_name, f\"Dropped unwanted columns: {dropped_columns}.\")\n",
    "\n",
    "    # 5. Split season_name Column if Applicable\n",
    "    if \"season_name\" in df.columns:\n",
    "        df.loc[:, \"season_name\"] = df[\"season_name\"].apply(\n",
    "            lambda x: x if isinstance(x, list) else [int(x)] if pd.notnull(x) else []\n",
    "        )\n",
    "        df.loc[:, \"season_name_initial\"] = df[\"season_name\"].apply(lambda x: min(x) if x else None)\n",
    "        df.loc[:, \"season_name_current\"] = df[\"season_name\"].apply(lambda x: max(x) if x else None)\n",
    "        df = df.drop(columns=[\"season_name\"])\n",
    "        log_change(dataset_name, \"Split 'season_name' into 'season_name_initial' and 'season_name_current'.\")\n",
    "\n",
    "    # 6. Foreign Key Validation\n",
    "    if dataset_name == \"mls/players/xgoals\" or dataset_name == \"mls/players/xpass\":\n",
    "        player_ids = set(dataframes[\"mls/players\"][\"player_id\"])\n",
    "        df = df[df[\"player_id\"].isin(player_ids)]\n",
    "        log_change(dataset_name, \"Validated and kept only valid player_id entries.\")\n",
    "\n",
    "    if dataset_name == \"mls/games\":\n",
    "        team_ids = set(dataframes[\"mls/teams\"][\"team_id\"])\n",
    "        df = df[df[\"home_team_id\"].isin(team_ids) & df[\"away_team_id\"].isin(team_ids)]\n",
    "        log_change(dataset_name, \"Validated and kept only valid team_id entries.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to clean and validate all datasets\n",
    "def clean_and_validate_all(dataframes):\n",
    "    for dataset_name, df in dataframes.items():\n",
    "        print(f\"Cleaning and validating {dataset_name}...\")\n",
    "        dataframes[dataset_name] = clean_and_validate_dataset(df, dataset_name, dataframes)\n",
    "    return dataframes\n",
    "\n",
    "# Run the cleaning and validation process\n",
    "dataframes = clean_and_validate_all(dataframes)\n",
    "\n",
    "# Display change logs\n",
    "print(\"Change Logs:\")\n",
    "for dataset, logs in change_logs.items():\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    for log in logs:\n",
    "        print(f\"  - {log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading AU_players...\n",
      "Uploading batch 0 to 999...\n",
      "Uploading AU_players_xgoals...\n",
      "Uploading batch 0 to 611...\n",
      "Uploading AU_players_xpass...\n",
      "Uploading batch 0 to 611...\n",
      "Uploading AU_players_salaries...\n",
      "Uploading batch 0 to 583...\n",
      "Uploading AU_teams...\n",
      "Uploading batch 0 to 29...\n",
      "Uploading AU_games...\n",
      "Uploading batch 0 to 999...\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "# Function to dynamically map Python data types to SQL data types\n",
    "def get_sql_data_type(dtype, column_name=None):\n",
    "    if column_name == \"player_id\":\n",
    "        return \"VARCHAR(50)\"  # Ensure player_id is a fixed-length string for indexing\n",
    "    elif column_name == \"primary_broad_position\" or column_name == \"primary_general_position\":\n",
    "        return \"VARCHAR(50)\"  # Make sure positions are fixed length for indexing\n",
    "    elif column_name == \"birth_date\":\n",
    "        return \"DATE\"\n",
    "    elif column_name == \"date_time_utc\":\n",
    "        return \"DATETIME\"\n",
    "    elif column_name in [\"base_salary\", \"guaranteed_compensation\"]:\n",
    "        return \"DECIMAL(12, 2)\"\n",
    "    elif pd.api.types.is_integer_dtype(dtype):\n",
    "        return \"INT\"\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return \"FLOAT\"\n",
    "    elif pd.api.types.is_object_dtype(dtype):\n",
    "        return \"NVARCHAR(255)\"  # Limit size for object (string) columns to make indexing possible\n",
    "    else:\n",
    "        return \"NVARCHAR(255)\"  # Default to NVARCHAR(255)\n",
    "\n",
    "# Main function to upload DataFrame to SQL Server\n",
    "def upload_to_sql_server(df, table_name, server, database, batch_size=5000):\n",
    "    conn = pyodbc.connect(f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;\")\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Drop table if it exists\n",
    "    cursor.execute(f\"IF OBJECT_ID('{table_name}', 'U') IS NOT NULL DROP TABLE {table_name}\")\n",
    "    conn.commit()\n",
    "\n",
    "    # Create table\n",
    "    create_query = f\"CREATE TABLE {table_name} ({', '.join([f'[{col}] {get_sql_data_type(df[col].dtype, col)}' for col in df.columns])})\"\n",
    "    cursor.execute(create_query)\n",
    "    conn.commit()\n",
    "\n",
    "    # Insert data in batches\n",
    "    placeholders = \", \".join([\"?\" for _ in df.columns])\n",
    "    insert_query = f\"INSERT INTO {table_name} VALUES ({placeholders})\"\n",
    "    for start in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[start:start + batch_size]\n",
    "        print(f\"Uploading batch {start} to {start + len(batch) - 1}...\")\n",
    "        try:\n",
    "            cursor.executemany(insert_query, batch.values.tolist())\n",
    "            conn.commit()\n",
    "            # Log change after each successful batch\n",
    "            log_change(table_name, f\"Successfully uploaded batch {start} to {start + len(batch) - 1}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading batch {start} to {start + len(batch) - 1}: {e}\")\n",
    "            problematic_rows = batch\n",
    "            print(problematic_rows.head())\n",
    "            conn.close()\n",
    "            return\n",
    "\n",
    "    # Create Clustered Index on player_id in mls/players\n",
    "    if \"player_id\" in df.columns:\n",
    "        cursor.execute(f\"CREATE CLUSTERED INDEX idx_player_id ON {table_name} (player_id)\")\n",
    "        conn.commit()\n",
    "\n",
    "    # Create Non-Clustered Indexes based on column names for each dataset\n",
    "    if table_name == \"AU_players\":\n",
    "        cursor.execute(f\"CREATE NONCLUSTERED INDEX idx_primary_broad_position ON {table_name} (primary_broad_position)\")\n",
    "        cursor.execute(f\"CREATE NONCLUSTERED INDEX idx_primary_general_position ON {table_name} (primary_general_position)\")\n",
    "        conn.commit()\n",
    "\n",
    "    if table_name == \"AU_players_xgoals\":\n",
    "        cursor.execute(f\"CREATE NONCLUSTERED INDEX idx_team_id_xgoals ON {table_name} (team_id)\")\n",
    "        cursor.execute(f\"CREATE NONCLUSTERED INDEX idx_goals_xgoals ON {table_name} (goals)\")\n",
    "        cursor.execute(f\"CREATE NONCLUSTERED INDEX idx_xgoals_plus_xassists ON {table_name} (xgoals_plus_xassists)\")\n",
    "        conn.commit()\n",
    "\n",
    "    if table_name == \"AU_players_xpass\":\n",
    "        cursor.execute(f\"CREATE NONCLUSTERED INDEX idx_team_id_xpass ON {table_name} (team_id)\")\n",
    "        cursor.execute(f\"CREATE NONCLUSTERED INDEX idx_attempted_passes_xpass ON {table_name} (attempted_passes)\")\n",
    "        cursor.execute(f\"CREATE NONCLUSTERED INDEX idx_pass_completion_percentage_xpass ON {table_name} (pass_completion_percentage)\")\n",
    "        conn.commit()\n",
    "\n",
    "    if table_name == \"AU_players_salaries\":\n",
    "        cursor.execute(f\"CREATE NONCLUSTERED INDEX idx_team_season_salary ON {table_name} (team_id, season_name_initial)\")\n",
    "        cursor.execute(f\"CREATE NONCLUSTERED INDEX idx_base_salary ON {table_name} (base_salary)\")\n",
    "        cursor.execute(f\"CREATE NONCLUSTERED INDEX idx_guaranteed_compensation ON {table_name} (guaranteed_compensation)\")\n",
    "        conn.commit()\n",
    "\n",
    "    if table_name == \"AU_teams\":\n",
    "        cursor.execute(f\"CREATE NONCLUSTERED INDEX idx_team_name ON {table_name} (team_name)\")\n",
    "        conn.commit()\n",
    "\n",
    "    if table_name == \"AU_games\":\n",
    "        cursor.execute(f\"CREATE NONCLUSTERED INDEX idx_home_team_id ON {table_name} (home_team_id)\")\n",
    "        cursor.execute(f\"CREATE NONCLUSTERED INDEX idx_away_team_id ON {table_name} (away_team_id)\")\n",
    "        cursor.execute(f\"CREATE NONCLUSTERED INDEX idx_date_time_utc ON {table_name} (date_time_utc)\")\n",
    "        conn.commit()\n",
    "\n",
    "    # Log completion of the dataset upload\n",
    "    log_change(table_name, f\"Data successfully uploaded and indexed in {table_name}\")\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "# Workflow to process and upload each DataFrame\n",
    "server_name = \"RAMSEY_BOLTON\\\\SQLEXPRESS\"\n",
    "database_name = \"SoccerProjects\"\n",
    "\n",
    "# Process and upload data\n",
    "for table_name, df in dataframes.items():\n",
    "    sql_table_name = table_name.replace(\"mls/\", \"AU_\").replace(\"/\", \"_\")  # Customize table naming\n",
    "    print(f\"Uploading {sql_table_name}...\")\n",
    "    upload_to_sql_server(df, sql_table_name, server_name, database_name, batch_size=5000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
